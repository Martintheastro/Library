from delta.tables import *
# from pyspark.sql import SparkSession
# import pandas as pd
from pyspark.sql.functions import *
 
# select data from Power BI semantic model in MDRLakehouse into dataframe - df:
df = spark.sql("SELECT * FROM fact_DQRules")
 
# Change column names to be able to save is as delta table in Tables folder of the Lakehouse (old name, new name). Lakehouse does not support %,".","(",")" in column names:
df=df.withColumnRenamed('old name', 'new name')\
    .withColumnRenamed('old name', 'new name')\
    .withColumn("ReportDate",df["Extract_timestamp"].substr(0, 10))\
    .withColumn("ReportDate",to_date("ReportDate"))


# when new column has to be renamed just add .withColumnRenamed('old name', 'new name')\, last row of this df. statement cant have "\"
 
# Create spark dataframe to be able to save it as delta table in lakehouse:
# spark_df = spark.createDataFrame(df)

 
# Save DataFrame as append to Delta Lake table in the Tables section of the default Lakehouse, append new records:
# if you want to change the table, change it here, run the code, it will create table and than another runs will append data
table_name = "fact_DQRulesHistorical" 
df.write.mode("append").format("delta").partitionedBy("ReportDate").save(f"Tables/{table_name}")

# display(df)

#########################################################################################################################################################################
# create date dimension:

from pyspark.sql.types import *
from delta.tables import*
from pyspark.sql.functions import col, dayofmonth, month, year, date_format

# Load data to the dataframe 
df = spark.read.table("MSExercise.fact_DQRulesHistorical")

DeltaTable.createIfNotExists(spark) \
     .tableName("MDRLakehouse.dim_date") \
     .addColumn("ReportDate", DateType()) \
     .addColumn("Day", IntegerType()) \
     .addColumn("Month", IntegerType()) \
     .addColumn("Year", IntegerType()) \
     .addColumn("mmmyyyy", StringType()) \
     .addColumn("yyyymm", StringType()) \
     .execute()


# Create dataframe for dimDate_gold
    
dim_date = df.dropDuplicates(["ReportDate"]).select(col("ReportDate"), \
         dayofmonth("ReportDate").alias("Day"), \
         month("ReportDate").alias("Month"), \
         year("ReportDate").alias("Year"), \
         date_format(col("ReportDate"), "MMM-yyyy").alias("mmmyyyy"), \
         date_format(col("ReportDate"), "yyyyMM").alias("yyyymm"), \
     ).orderBy("ReportDate")

 # Display the first 10 rows of the dataframe to preview your data

display(dim_date.head(10))


# df = spark.sql("SELECT * FROM MDRLakehouse.dim_date LIMIT 1000")
# display(df)


