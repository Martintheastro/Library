from pyspark.sql import DataFrame
from pyspark.sql.functions import sum,concat,col, lit
def dataframe_grouped_Accounts(List_KPIReporting,report_group) -> DataFrame:
    
    df_kpi_results = []

    for kpi_name in List_KPIReporting:

        kpi = kpi_name['KPI']

        for kpi_group in kpi_name['Breakdown']:

            Metric = kpi_group['Metric']
            List_Accounts = kpi_group['List_Accounts']
            df =  kpi_group['Source']
            df_group_results = []

            for entity_list, label in report_group:
        
                matching_accounts = df.filter((col("Account").isin(List_Accounts)) & (col("Entity").isin(entity_list)))
    
                df_group = matching_accounts\
                    .groupBy('Account','Time','View','Scenario','UD5')\
                    .agg(sum(col('Corrected_Amount')).alias('Value'))\
                    .withColumn('Metric',lit(Metric))\
                    .withColumn('Entity',lit(label))

                df_group_results.append(df_group)

        df_kpi_results.append(df_group_results)

    return df_kpi_results
